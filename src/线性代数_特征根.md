# 定义

$$
T\vec{v} = \lambda\vec{v}
$$

- 特征向量
  在矩阵变换中 有部分特殊的向量在变换前后方向保持不变,叫做特征向量

- 特征值
  特征向量在变换中被拉伸和压缩的倍数

# 计算

$$
\begin{align}
T\vec{v}  & = \lambda \vec{v} \\
T\vec{v} -  \lambda I\vec{v} & = 0 \\
(T- \lambda I)\vec{v} & = 0 \\
det(T- \lambda I) & = 0
\end{align}
$$

# 例子

$$
\begin{align}
T & = \begin{vmatrix}
3 & 1 \\
0 & 2
\end{vmatrix} \\
det(T- \lambda I) & = det(
\begin{vmatrix}
3-\lambda & 1 \\
0 & 2-\lambda
\end{vmatrix}
) \\
det(T- \lambda I) & = (3-\lambda)(2-\lambda) \\
\lambda = 3 &,  \vec{v} = k\begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
\lambda = 2 &, \vec{v} = k\begin{bmatrix} 1 \\ -1 \end{bmatrix}
\end{align}
$$

## 特例

### 伸缩变换

$$
\begin{align}
T =&
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix} \\
(2-\lambda)^2 =& \quad 0 \\
\lambda =2 , \vec{v} =& \quad \forall \vec{v} \in R^2
\end{align}
$$

### 旋转变换

$$
\begin{align}
T =&
\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix} \\
\begin{vmatrix}
0-\lambda & -1 \\
1 & 0-\lambda
\end{vmatrix} =& 0 \\
\lambda^2 + 1 =& 0
\end{align}
$$

旋转会改变所有向量的方向

所以没有特征根也没有特征向量

### 剪切变换

$$
\begin{align}

T = &
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}\\
\begin{vmatrix}
1-\lambda & 1 \\
0 & 1-\lambda
\end{vmatrix} =& 0 \\
(1-\lambda)^2 =& 0 \\
\lambda = 1 ,&\vec{v} = k\begin{bmatrix} 1 \\ 0 \end{bmatrix}
\end{align}
$$

只有 x 轴方向的向量在变换后保持方向不变

其他所有向量都会被"剪切"而改变方向

## 正交基变换

$$
\begin{align}
T =&
\begin{bmatrix}
1 & 0 \\
0 & 2
\end{bmatrix} \\
(1-\lambda)(2-\lambda) =& 0 \\
\lambda = 1 ,& \vec{v} =
\begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
\lambda = 2 ,& \vec{v} =
\begin{bmatrix} 0 \\ 1 \end{bmatrix}
\end{align}
$$

在 x 轴方向放大 1 倍（保持不变)

在 y 轴方向放大 2 倍

## python 求特征根 特征向量

```python
import numpy as np

# 定义矩阵
A = np.array([
    [1, 0],
    [0, 2]
])

# 计算特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(A)

print("特征值:")
print(eigenvalues)
print("\n特征向量:")
print(eigenvectors)

# 验证特征值和特征向量
print("\n验证:")
for i in range(len(eigenvalues)):
    lambda_i = eigenvalues[i]
    v_i = eigenvectors[:, i]
    print(f"特征值 {lambda_i}:")
    print(f"A * v = {np.dot(A, v_i)}")
    print(f"lambda * v = {lambda_i * v_i}")
    print("---")

```

# 应用

1. 主成分分析 (PCA)

- 用于数据降维
- 通过计算协方差矩阵的- 特征值和特征向量
- 特征值大的方向代表数- 据变化最大的方向

2. 图像处理

- 通过奇异值分解(SVD)
- 可以保留主要特征去除噪声

3. 网络分析

- 特征向量中心性（Eigenvector Centrality）

- 可以找出网络中最重要的节点

4. 推荐系统

- 降维和特征提取

  - 将高维的用户-物品交互矩阵降维到低维空间
  - 提取用户和物品的潜在特征

- 相似度计算

  - 通过特征向量计算用户之间或物品之间的相似度
  - 用于基于邻域的推荐

- 预测评分

  - 使用特征向量的点积来预测用户对物品的评分
  - 实现个性化推荐

- 处理稀疏数据

  - 通过矩阵分解填补缺失值
  - 提高推荐的准确性

- 可解释性
  - 特征向量可以解释为用户偏好或物品特性
  - 帮助理解推荐的原因
