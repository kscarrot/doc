# 特征分解

## 定义

矩阵的特征分解（Eigen Decomposition）

是将一个方阵分解为特征值和特征向量的过程

对于一个 `n×n` 的方阵 A，其特征分解可以表示为：

$$A = Q\Lambda Q^{-1}$$

其中：

- `Q` 是由 `A` 的特征向量组成的矩阵
- $\Lambda$ 是对角矩阵，对角线上的元素是 `A` 的特征值
- $Q^{-1}$ 是 `Q` 的逆矩阵

## 推导

### 线性变换的表示：

矩阵$A$作用在特征向量$\vec{v}$上时，只是对$\vec{v}$进行缩放（由特征值$\lambda$决定）

这意味着$A$可以通过其特征向量和特征值来表示。

### 对角化的条件：

如果矩阵$A$有$n$个线性无关的特征向量，则$A$是可对角化的。

这是因为$P$是可逆的（由线性无关性保证），从而可以构造出$P^{-1}$。

### 几何解释：

特征分解将矩阵$A$的作用分解为三个步骤：

1. 通过$P^{-1}$将坐标系变换到特征向量的基底
2. 通过$D$进行缩放
3. 通过$P$将坐标系变换回原来的基底。

## 特征分解示例

考虑一个 2×2 的矩阵：

$$
A = \begin{bmatrix}
7 & -2 \\
6 & 0
\end{bmatrix}
$$

### 1. 求解特征值

计算特征方程：

$$
\det(A - \lambda I) = \begin{vmatrix}
7 - \lambda & -2 \\
6 & -\lambda
\end{vmatrix} = (7 - \lambda)(-\lambda) + 12 = 0
$$

展开得到：

$$
\lambda^2 - 7\lambda + 12 = 0
$$

解得特征值：

$$
\lambda_1 = 3, \quad \lambda_2 = 4
$$

### 2. 求解特征向量

对于 $\lambda_1 = 3$：

$$
(A - 3I)\vec{v}_1 = \begin{bmatrix}
4 & -2 \\
6 & -3
\end{bmatrix}\vec{v}_1 = 0
$$

解得特征向量：

$$
\vec{v}_1 = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

对于 $\lambda_2 = 4$：

$$
(A - 4I)\vec{v}_2 = \begin{bmatrix}
3 & -2 \\
6 & -4
\end{bmatrix}\vec{v}_2 = 0
$$

解得特征向量：

$$
\vec{v}_2 = \begin{bmatrix}
2 \\
3
\end{bmatrix}
$$

### 2.1 标准化特征向量

对特征向量进行标准化（单位化）：

对于 $\vec{v}_1$：

$$
\|\vec{v}_1\| = \sqrt{1^2 + 2^2} = \sqrt{5}
$$

标准化后的特征向量：

$$
\vec{u}_1 = \frac{\vec{v}_1}{\|\vec{v}_1\|} = \begin{bmatrix}
\frac{1}{\sqrt{5}} \\
\frac{2}{\sqrt{5}}
\end{bmatrix}
$$

对于 $\vec{v}_2$：

$$
\|\vec{v}_2\| = \sqrt{2^2 + 3^2} = \sqrt{13}
$$

标准化后的特征向量：

$$
\vec{u}_2 = \frac{\vec{v}_2}{\|\vec{v}_2\|} = \begin{bmatrix}
\frac{2}{\sqrt{13}} \\
\frac{3}{\sqrt{13}}
\end{bmatrix}
$$

### 3. 构造特征分解

将标准化后的特征向量作为列向量构造矩阵 $Q$：

$$
Q = \begin{bmatrix}
\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{13}} \\
\frac{2}{\sqrt{5}} & \frac{3}{\sqrt{13}}
\end{bmatrix}
$$

构造对角矩阵 $\Lambda$：

$$
\Lambda = \begin{bmatrix}
3 & 0 \\
0 & 4
\end{bmatrix}
$$

计算 $Q$ 的逆矩阵（由于 $Q$ 是正交矩阵，$Q^{-1} = Q^T$）：

$$
Q^{-1} = Q^T = \begin{bmatrix}
\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{5}} \\
\frac{2}{\sqrt{13}} & \frac{3}{\sqrt{13}}
\end{bmatrix}
$$

最终得到特征分解：

$$
A = Q\Lambda Q^{-1} = \begin{bmatrix}
\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{13}} \\
\frac{2}{\sqrt{5}} & \frac{3}{\sqrt{13}}
\end{bmatrix}
\begin{bmatrix}
3 & 0 \\
0 & 4
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{5}} \\
\frac{2}{\sqrt{13}} & \frac{3}{\sqrt{13}}
\end{bmatrix}
$$



## 代码验证

```python
import numpy as np

# 定义矩阵
A = np.array([[7, -2],
              [6, 0]])

# 计算特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(A)

# 构造对角矩阵
Lambda = np.diag(eigenvalues)

# 特征向量矩阵 Q:
# [[0.5547002  0.4472136 ]
#  [0.83205029 0.89442719]]

# 对角矩阵 Lambda:
# [[4. 0.]
#  [0. 3.]]
```

# SVD 分解

SVD 分解(Singular Value Decomposition)是一种重要的矩阵分解方法

对于任意一个 m×n 的实矩阵 A 都可以分解为三个矩阵的乘积:

$$A = U\Sigma V^T$$

其中:

- `U`是`m×m`正交矩阵,其列向量称为左奇异向量
- `Σ`是`m×n`对角矩阵,对角线上的元素称为奇异值
- `V`是`n×n`正交矩阵,其列向量称为右奇异向量

## 推导步骤

### 1. 计算$A^TA$和$AA^T$

首先计算$A^TA$和$AA^T$，这两个矩阵都是对称矩阵：

- $A^TA$是 n×n 矩阵
- $AA^T$是 m×m 矩阵

### 2. 计算特征值和特征向量

对$A^TA$和$AA^T$进行特征分解：

- $A^TA$的特征向量组成矩阵$V$
- $AA^T$的特征向量组成矩阵$U$
- 特征值的平方根就是奇异值，即 $\sigma_i = \sqrt{\lambda_i}$，这些奇异值组成对角矩阵 $\Sigma$

### 3. 构造奇异值矩阵

将特征值按从大到小排序，取其平方根作为奇异值，构造$\Sigma$矩阵：

$$
\Sigma = \begin{bmatrix}
\sigma_1 & 0 & \cdots & 0 \\
0 & \sigma_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_r \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0
\end{bmatrix}
$$

其中$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$，r 是矩阵 A 的秩。

## 代码验证

```python
import numpy as np

# 定义矩阵
A = np.array([[4, 0],
              [3, -5]])

# 计算SVD分解
U, S, Vt = np.linalg.svd(A)

# 打印结果
print("U矩阵:")
print(U)
#[[-0.4472136  -0.89442719]
# [-0.89442719  0.4472136 ]]
print("\n奇异值:")
print(S)
# [6.32455532 3.16227766]
print("\nV转置矩阵:")
print(Vt)
#[[-0.70710678  0.70710678]
# [-0.70710678 -0.70710678]]

# 验证分解
Sigma = np.zeros_like(A)
Sigma[:len(S), :len(S)] = np.diag(S)
A_reconstructed = U @ Sigma @ Vt
print("\n重构误差:")
print(np.linalg.norm(A - A_reconstructed))
# 0.36286387936610587
```

# 消去次要成分示例

```python
import numpy as np
# 创建一个4×4的测试矩阵
A_test = np.array([[1, 2, 3, 4],
                   [2, 4, 6, 8],
                   [3, 6, 9, 11],
                   [4, 8, 12, 15]])
# SVD分解
U_test, S_test, Vt_test = np.linalg.svd(A_test)


# 只保留最大的奇异值(消去次要成分)
S_reduced = np.copy(S_test)
S_reduced[1:] = 0  # 将第二个及以后的奇异值置为0

# 构造简化后的Sigma矩阵
Sigma_reduced = np.zeros_like(A_test, dtype=float)
Sigma_reduced[:len(S_reduced), :len(S_reduced)] = np.diag(S_reduced)

# 重构矩阵
A_reduced = U_test @ Sigma_reduced @ Vt_test
```
演示SVD分解消去次要成分:

原始奇异值:
$$\sigma = [29.08, 0.427, 9.25×10^{-16}, 9.99×10^{-17}]$$

原始矩阵:

$$
A = \begin{bmatrix}
1 & 2 & 3 & 4 \\
2 & 4 & 6 & 8 \\
3 & 6 & 9 & 11 \\
4 & 8 & 12 & 15
\end{bmatrix}
$$

只保留最大奇异值重构的矩阵:

$$
A_{reduced} = \begin{bmatrix}
1.03 & 2.06 & 3.09 & 3.88 \\
2.06 & 4.12 & 6.19 & 7.77 \\
2.96 & 5.92 & 8.88 & 11.15 \\
3.99 & 7.98 & 11.97 & 15.04
\end{bmatrix}
$$

重构误差:
$$\|A - A_{reduced}\| = 0.427$$

相对误差:
$$\frac{\|A - A_{reduced}\|}{\|A\|} = 0.0147$$

降维后的表示:

U的第一列 ($\vec{u_1}$):

$$
\vec{u_1} = \begin{bmatrix}
-0.188 \\
-0.376 \\
-0.540 \\
-0.729
\end{bmatrix}
$$

最大的奇异值 ($\sigma_1$):

$$\sigma_1 = 29.083$$

V^T的第一行 ($\vec{v_1}^T$):

$$
\vec{v_1}^T = \begin{bmatrix}
-0.188 & -0.377 & -0.565 & -0.710
\end{bmatrix}
$$

降维后的矩阵 = $\vec{u_1} \sigma_1 \vec{v_1}^T$:


$$
A_{reduced} = \begin{bmatrix}
1.03 & 2.06 & 3.09 & 3.88 \\
2.06 & 4.12 & 6.19 & 7.77 \\
2.96 & 5.92 & 8.88 & 11.15 \\
3.99 & 7.98 & 11.97 & 15.04
\end{bmatrix}
$$